{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SRi2RUYH_aD",
        "outputId": "112c0e2d-dc56-4536-9311-b5bebc40c8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU\n",
            "mkdir: cannot create directory ‘models’: File exists\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pprint as pp\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import special_ortho_group\n",
        "from scipy.integrate import solve_ivp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "\n",
        "#get device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    print(\"CPU\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "!mkdir models\n",
        "#get files\n",
        "files = ['X.bin','dX.bin','dynamicalsystems.py',]\n",
        "for name in files:\n",
        "    with requests.get('https://raw.githubusercontent.com/audrey163/Autoencoder/main/'+name, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(name, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192): \n",
        "                f.write(chunk)\n",
        "\n",
        "from dynamicalsystems import SimplePendulum, Lorenz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-wqeFm8VOuW7"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'architecture' : { \n",
        "        'input_dimension' : 10,\n",
        "        'latent_dimension' : 2 ,\n",
        "        'SIND-y' : {}\n",
        "    },\n",
        "    'optimization' : { \n",
        "        'learn_rate' : 3e-4,\n",
        "        'loss_reg' : {\n",
        "            'X' : 1,\n",
        "            'SINDy' : 1,\n",
        "            'dX' : 1, #dX is regularization is 0 because dX_pred = None\n",
        "            'Xi1' : 10,\n",
        "            'Xi2' : 0,\n",
        "            'zero-pole' : True #zero-pole is my custom loss function that uses |Xi|+1/|Xi| insted of |Xi| this adds a pole at zero when\n",
        "        }\n",
        "    },\n",
        "    'training' : {\n",
        "        'epochs' : 16000,\n",
        "        'batch_size' : 2000,\n",
        "        'save_freq' : 10,\n",
        "        'load_weghts_from' : ':latest' # 'random.init', ':latest', None, 'models/FILENAME'\n",
        "    },\n",
        "    'inputs' : {\n",
        "        'total_samples' : 2000,\n",
        "    },\n",
        "    'outputs' : {\n",
        "        'loss' : True,\n",
        "        'sindy_show' : False,\n",
        "    },\n",
        "    'sindy' : {\n",
        "        'max_degree' : 2,\n",
        "        'cross_terms' : True,\n",
        "    }\n",
        "}\n",
        "datasets = []\n",
        "for i in range(0,9):\n",
        "    #dataset = Lorenz(time_steps=params['inputs']['total_samples'],embed_params={'embed_dim' : params['architecture']['input_dimension'], 'mu' : 0,'sigma' : 0,'mu1' : 0,'sigma1' : 0,'mat' : 'RANDN'})\n",
        "    dataset = SimplePendulum(g=9.8,l=2,mu=0.5,theta0=np.pi/2,time_steps=params['inputs']['total_samples'],embed_params={'embed_dim' : params['architecture']['input_dimension'], 'mu' : 0,'sigma' : 0,'mu1' : 0,'sigma1' : 0,'mat' : 'RANDN'})\n",
        "    dataset.x0 = (np.pi/3+np.random.randn()*np.pi/6,np.abs(np.random.randn()))\n",
        "    dataset.X, dataset.dX = dataset.solve()\n",
        "    dataset.embed()\n",
        "    total_samples = len(dataset)\n",
        "\n",
        "    #X,dX = torch.Tensor(torch.load(\"X.bin\").T), torch.Tensor(torch.load(\"dX.bin\").T)\n",
        "    ds = {'X' : torch.Tensor(dataset.Z.T),'dX' : torch.Tensor(dataset.dZ.T)} #GPU bug just going wo work with the values directoly not use torch.utils.data.Dataset inharitance\n",
        "    ds['X'].to(device)\n",
        "    ds['dX'].to(device)\n",
        "    datasets.append(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "41ONfKD_dylN"
      },
      "outputs": [],
      "source": [
        "class PolynomialLibrary:\n",
        "    def __init__(self, max_degree=params['sindy']['max_degree'], cross_terms=params['sindy']['cross_terms']):\n",
        "        self.max_degree = max_degree\n",
        "        self.cross_terms = cross_terms\n",
        "    def get_candidates(self, dim, feature_names):\n",
        "        self.feature_names = feature_names\n",
        "        return [self.__polynomial(degree_sequence) \n",
        "                    for degree in range(1,self.max_degree+1)\n",
        "                        for degree_sequence in self.__get_degree_sequences(degree, dim)]\n",
        "    def __polynomial(self, degree_sequence):\n",
        "        def fn(X):\n",
        "            terms = torch.stack( tuple(X[:,i]**d for i,d in enumerate(degree_sequence)), axis=1 )\n",
        "            return torch.prod(terms, dim=1)\n",
        "        fn_name = ' '.join(self.__display_term(self.feature_names[i],d) for i,d in enumerate(degree_sequence) if d)    \n",
        "        return (fn, fn_name)   \n",
        "    def __display_term(self, feature_name, d):\n",
        "        if d == 1:\n",
        "            return f'{feature_name}'\n",
        "        return f'{feature_name}^{d}'\n",
        "    def __get_degree_sequences(self, degree, num_terms):\n",
        "        if num_terms == 1:  return [[degree]]\n",
        "        if degree == 0:     return [[0 for _ in range(num_terms)]]\n",
        "        res = []\n",
        "        for d in reversed(range(degree+1)):\n",
        "            for seq in self.__get_degree_sequences(degree-d, num_terms-1):\n",
        "                res.append([d, *seq])\n",
        "        return res\n",
        "class TrigLibrary:\n",
        "    def __init__(self):\n",
        "        self.max_freq = 1\n",
        "    def get_candidates(self, dim, feature_names):\n",
        "        self.feature_names = feature_names\n",
        "        return [trig(i) for trig in [self.__sin, self.__cos] for i in range(dim)]\n",
        "    def __sin(self,i):\n",
        "        fn = lambda X: torch.sin(X[:,i])\n",
        "        fn_name = f'sin({self.feature_names[i]})'\n",
        "        return (fn, fn_name)\n",
        "    def __cos(self,i):\n",
        "        fn = lambda X: torch.cos(X[:,i])\n",
        "        fn_name = f'cos({self.feature_names[i]})'\n",
        "        return (fn,fn_name)\n",
        "class Theta:\n",
        "    def __init__(self,):\n",
        "        self.num_snapshots, self.num_features  = params['training']['batch_size'], params['architecture']['latent_dimension']\n",
        "        self.feature_names = [f'x{i+1}' for i in range(self.num_features)]\n",
        "        self.candidate_terms = [ lambda x: torch.ones(self.num_snapshots).to(device) ]\n",
        "        self.candidate_names = ['1']\n",
        "        self.libs = [ PolynomialLibrary(max_degree=1), TrigLibrary() ]\n",
        "        for lib in self.libs:\n",
        "            lib_candidates = lib.get_candidates(self.num_features, self.feature_names)\n",
        "            for term, name in lib_candidates:\n",
        "                self.candidate_terms.append(term)\n",
        "                self.candidate_names.append(name)\n",
        "    def theta(self,X):\n",
        "         return torch.stack(tuple(f(X) for f in self.candidate_terms), axis=1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZEdOcCXOlVM",
        "outputId": "bf74a6f6-dcba-4bf6-bc10-588202deda2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Theta.theta of <__main__.Theta object at 0x7fb0525c3c50>>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "theta = Theta()\n",
        "theta.theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2m3zuzqb8oiw"
      },
      "outputs": [],
      "source": [
        "class FullSINDyAutoencoder(nn.Module):\n",
        "    def __init__(self,params=None):\n",
        "        super().__init__()\n",
        "        self.params = params if params is not None else self.get_params()\n",
        "        self.Theta = Theta()\n",
        "        self.theta = Theta.theta\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.params['architecture']['input_dimension'], self.params['architecture']['latent_dimension'])\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.params['architecture']['latent_dimension'],self.params['architecture']['input_dimension'])\n",
        "        )\n",
        "        self.SINDy_layer = nn.Sequential(nn.Linear(len(self.Theta.candidate_terms),self.params['architecture']['latent_dimension'],bias=False))\n",
        "        self.encoder.to(device)\n",
        "        self.decoder.to(device)\n",
        "        self.SINDy_layer.to(device)\n",
        "    def forward(self, X, dX):\n",
        "        Z = self.encoder(X.to(device)).to(device)\n",
        "        return self.forward1(X,dX,Z)\n",
        "\n",
        "    def forward1(self, X, dX,Z):\n",
        "        X_pred = self.decoder(Z.to(device)).to(device)\n",
        "        return self.forward2(X,dX,Z,X_pred)\n",
        "\n",
        "    def forward2(self,X,dX,Z,X_pred):\n",
        "        dZ = self.translate_derivative(X, dX, self.encoder)\n",
        "        return self.forward3(X,dX,Z,X_pred,dZ)\n",
        "    def forward3(self,X,dX,Z,X_pred,dZ):\n",
        "        #now we must compute dZ pred\n",
        "        dZ_pred = torch.matmul(theta.theta(Z.to(device)).to(device),self.SINDy_layer[0].weight.T.to(device)).to(device)#######################################################################\n",
        "        # Xi is just sindy weights\n",
        "        Xi = self.SINDy_layer[0].weight.to(device)\n",
        "        return self.forward4(Z, X_pred, dZ, dZ_pred, Xi)\n",
        "    def forward4(self,Z, X_pred, dZ, dZ_pred, Xi):\n",
        "        dX_pred = self.translate_derivative(Z,dZ_pred,self.decoder).to(device)\n",
        "        return Z, X_pred, dZ, dZ_pred, Xi, dX_pred\n",
        "    def closure(self,X,dX,Z, X_pred, dZ, dZ_pred, Xi,dX_pred):\n",
        "        Lx = torch.nn.functional.mse_loss(X.to(device),X_pred.to(device)).to(device) * self.params['optimization']['loss_reg']['X']\n",
        "        Ly = torch.nn.functional.mse_loss(dZ,dZ_pred).to(device) * self.params['optimization']['loss_reg']['SINDy']\n",
        "        Ldx = torch.nn.functional.mse_loss(dX,dX_pred).to(device) * self.params['optimization']['loss_reg']['dX']\n",
        "        if self.params['optimization']['loss_reg']['zero-pole']:\n",
        "            Li = self.never_zero_loss(Xi).to(device)\n",
        "        else:\n",
        "            Li = torch.abs(torch.sum(torch.abs(Xi)).to(device)-1)\n",
        "        Li *= self.params['optimization']['loss_reg']['Xi1']\n",
        "        return Lx.to(device) + Ly.to(device) + Li.to(device)\n",
        "    def translate_derivative(self, X, X_dot, network):\n",
        "        parameters = []\n",
        "        for layer in network:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                parameters.append((layer.weight, layer.bias))\n",
        "        l, dl = X, X_dot\n",
        "        relu = nn.ReLU()\n",
        "        for i, (w,b) in enumerate(parameters):\n",
        "            l = l @ w.T + b\n",
        "            dl = dl @ w.T\n",
        "            if i < len(parameters) - 1:\n",
        "                dl = (l > 0).float() * dl\n",
        "                l = relu(l)\n",
        "        return dl\n",
        "    def train(self,dataset,dataloader,params=None):\n",
        "        if params is not None:\n",
        "            self.params = params\n",
        "        optimizer = torch.optim.Adam(self.parameters(),lr=self.params['optimization']['learn_rate'])\n",
        "        losses = []\n",
        "        total_samples = len(dataset)\n",
        "        n_iter = math.ceil(total_samples / self.params['training']['batch_size'])\n",
        "        for epoch in range(self.params['training']['epochs']):\n",
        "            print(f'Epoch {str(epoch)}')\n",
        "            l = []\n",
        "            for i, (X, dX) in enumerate(dataloader):\n",
        "                X.to(device)\n",
        "                dX.to(device)\n",
        "                loss = self.closure(X,dX)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                l.append(loss.item())\n",
        "            losses.append(np.mean(np.array(l)))\n",
        "            print(loss.item())\n",
        "        plt.plot(np.log(np.array(losses)))\n",
        "        plt.title(\"log loss\")\n",
        "    def never_zero_loss(self,Xi):\n",
        "        x = torch.sum(torch.abs(Xi))\n",
        "        #this is aproximatly lim_x->infty (x-1/x)/x = (1+0)/1 = 1\n",
        "        #also note that this is very simular to |x| its just that when |x| is small it becomes very large |0|+1/|0| = infinity\n",
        "        return x + 1/x # the derivatative is 1-1/x^2 when x is far from zero this function is very cloase to x but when x is close to zero it becomes very large hence the optimization \n",
        "    def clip(self,epsilon):\n",
        "        with torch.no_grad():\n",
        "            self.SINDy_layer[0].weight.to(device)\n",
        "            self.SINDy_layer[0].weight[torch.abs(self.SINDy_layer[0].weight) < epsilon] = 0.0\n",
        "    def show(self):\n",
        "        rows = torch.Tensor(self.SINDy_layer[0].weight).tolist()\n",
        "        equations = [[round(coeff,7) for coeff in row] for row in rows]\n",
        "        for i,eq in enumerate(equations):\n",
        "            x = f'z{i+1}'\n",
        "            rhs = ' + '.join(f'{coeff} {name}' for coeff, name in zip(eq, self.Theta.candidate_names))\n",
        "            print(f'({x}.) = {rhs}.')   \n",
        "    def get_params(self):\n",
        "        import json\n",
        "        with open('FullSINDyAutoencoder.conf') as json_file:\n",
        "              return json.load(json_file)\n",
        "    def save(self):\n",
        "        import datetime\n",
        "        timestamp = str(datetime.datetime.now())\n",
        "        PATH = 'models/' + timestamp\n",
        "        torch.save(self.state_dict(),PATH)\n",
        "        print(f\"Saved model to : {PATH}\")\n",
        "\n",
        "    def load(self,state_dict_PATH=None):\n",
        "        if state_dict_PATH is not None:\n",
        "            if state_dict_PATH == ':latest':\n",
        "                from os import listdir\n",
        "                from os.path import isfile, join\n",
        "                onlyfiles = [f for f in listdir('models/') if isfile(join('models/', f))]\n",
        "                onlyfiles.sort(reverse=True)\n",
        "                state_dict_PATH = 'models/' + onlyfiles[0]\n",
        "            self.load_state_dict(torch.load(state_dict_PATH))\n",
        "            print(f\"Using Model: {state_dict_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUvcO4ugXy8C",
        "outputId": "77cab867-21bc-4fb7-d13e-cbb1ba1be580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16000/16000 [01:04<00:00, 248.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(z1.) = 0.0 1 + 0.0 x1 + -0.6108469 x2 + 0.0 sin(x1) + -0.000838 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n",
            "(z2.) = 0.0 1 + 0.3947148 x1 + 0.0 x2 + 0.0003671 sin(x1) + 0.0 sin(x2) + -0.0001062 cos(x1) + 0.0 cos(x2).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16000/16000 [01:03<00:00, 250.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(z1.) = 0.0 1 + 0.0 x1 + -0.4881885 x2 + 0.0 sin(x1) + -0.0001685 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n",
            "(z2.) = 0.0 1 + 0.5177047 x1 + 0.0 x2 + 0.0003189 sin(x1) + 0.0 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16000/16000 [01:04<00:00, 249.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(z1.) = 0.0 1 + 0.0 x1 + 0.5372394 x2 + -0.0001253 sin(x1) + 0.0003909 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n",
            "(z2.) = 0.0 1 + -0.4692742 x1 + 0.0 x2 + -0.0005341 sin(x1) + 0.0 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16000/16000 [01:02<00:00, 255.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(z1.) = 0.0 1 + 0.0 x1 + -0.4671268 x2 + 0.0 sin(x1) + -0.0004907 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n",
            "(z2.) = 0.0 1 + 0.5383486 x1 + 0.0 x2 + 0.0005247 sin(x1) + 0.0 sin(x2) + -0.0001001 cos(x1) + 0.0 cos(x2).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16000/16000 [01:03<00:00, 253.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(z1.) = 0.0 1 + 0.0 x1 + -0.4241835 x2 + 0.0 sin(x1) + -0.0003281 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n",
            "(z2.) = 0.0 1 + 0.5810438 x1 + -0.0001187 x2 + 0.0006402 sin(x1) + 0.0 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16000/16000 [01:01<00:00, 260.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(z1.) = 0.0 1 + 0.0 x1 + -0.6273953 x2 + 0.0 sin(x1) + -0.0006527 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n",
            "(z2.) = 0.0 1 + 0.3785569 x1 + 0.0 x2 + 0.0005522 sin(x1) + 0.0 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16000/16000 [01:04<00:00, 249.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(z1.) = 0.0 1 + 0.0 x1 + 0.4875655 x2 + -0.0001685 sin(x1) + 0.0006106 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n",
            "(z2.) = 0.0 1 + -0.5182764 x1 + 0.0 x2 + -0.0001434 sin(x1) + 0.0 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16000/16000 [01:03<00:00, 252.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(z1.) = 0.0 1 + 0.0 x1 + 0.5492883 x2 + 0.0 sin(x1) + 0.0005601 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n",
            "(z2.) = 0.0 1 + -0.4567253 x1 + 0.0 x2 + -0.0001325 sin(x1) + 0.0 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16000/16000 [01:03<00:00, 253.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(z1.) = 0.0 1 + 0.0 x1 + 0.4513766 x2 + 0.0 sin(x1) + 0.0 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n",
            "(z2.) = 0.0 1 + -0.5534143 x1 + 0.0 x2 + -0.0006925 sin(x1) + 0.0 sin(x2) + 0.0 cos(x1) + 0.0 cos(x2).\n"
          ]
        }
      ],
      "source": [
        "models = []\n",
        "for i,d in enumerate(datasets):\n",
        "    model = FullSINDyAutoencoder(params)\n",
        "    models.append(model)\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=params['optimization']['learn_rate'])\n",
        "    losses = []\n",
        "    for epoch in tqdm(range(params['training']['epochs'])):\n",
        "        #print(f'Epoch {str(epoch)}')\n",
        "        l = []\n",
        "        Z, X_pred, dZ, dZ_pred, Xi, dX_pred = model.forward(d['X'],d['dX'])\n",
        "        loss = model.closure(d['X'],d['dX'],Z, X_pred, dZ, dZ_pred, Xi,dX_pred)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.clip(1e-4)\n",
        "        l.append(loss.item())\n",
        "        losses.append(np.mean(np.array(l)))\n",
        "        #print(loss.item())\n",
        "        if epoch % 5 == 0 and epoch > 0:\n",
        "            plt.plot(np.log(np.array(losses)))\n",
        "            plt.title(\"log loss\")\n",
        "    model.show()\n",
        "    Z = model.encoder(d['X'])\n",
        "    plt.plot(Z[:,0].detach().numpy(),Z[:,1].detach().numpy())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Autoencoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}